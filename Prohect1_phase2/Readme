Training model using pretrained word embeddings
Train on 8000 samples, validate on 10000 samples
Epoch 1/10
8000/8000 [==============================] - 1s 80us/step - loss: 0.1848 - acc: 0.9460 - val_loss: 1.2929 - val_acc: 0.6704
Epoch 2/10
8000/8000 [==============================] - 0s 50us/step - loss: 0.1790 - acc: 0.9436 - val_loss: 1.2958 - val_acc: 0.6728
Epoch 3/10
8000/8000 [==============================] - 0s 50us/step - loss: 0.1752 - acc: 0.9457 - val_loss: 1.2752 - val_acc: 0.6715
Epoch 4/10
8000/8000 [==============================] - 0s 38us/step - loss: 0.1710 - acc: 0.9480 - val_loss: 1.1804 - val_acc: 0.6707
Epoch 5/10
8000/8000 [==============================] - 0s 41us/step - loss: 0.1717 - acc: 0.9489 - val_loss: 1.3768 - val_acc: 0.6755
Epoch 6/10
8000/8000 [==============================] - 0s 42us/step - loss: 0.1721 - acc: 0.9476 - val_loss: 1.5451 - val_acc: 0.6732
Epoch 7/10
8000/8000 [==============================] - 0s 39us/step - loss: 0.1769 - acc: 0.9464 - val_loss: 1.2589 - val_acc: 0.6679
Epoch 8/10
8000/8000 [==============================] - 0s 39us/step - loss: 0.1674 - acc: 0.9489 - val_loss: 1.3063 - val_acc: 0.6697
Epoch 9/10
8000/8000 [==============================] - 0s 45us/step - loss: 0.1634 - acc: 0.9517 - val_loss: 1.4411 - val_acc: 0.6712
Epoch 10/10
8000/8000 [==============================] - 0s 38us/step - loss: 0.1726 - acc: 0.9483 - val_loss: 1.3236 - val_acc: 0.6715

 


 
