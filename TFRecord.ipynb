{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TFRecord.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/krishnagorrepati/DeepLearningProjects/blob/master/TFRecord.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1JXGdBhswhCN",
        "colab_type": "text"
      },
      "source": [
        "# TFRecords\n",
        "*   If you are working with large datasets, using a binary file format for storage of your data can have a significant impact on the performance of your import pipeline\n",
        " and as a consequence on the training time of your model. \n",
        "*   Binary data takes up less space on disk, takes less time to copy and can be read much more efficiently from disk.\n",
        "\n",
        "*   it makes it easy to combine multiple datasets and integrates seamlessly with the data import and preprocessing functionality provided by the library\n",
        "*   Especially for datasets that are too large to be stored fully in memory this is an advantage as only the data that is required at the time (e.g. a batch) is loaded from disk and then processed.\n",
        "*   Another major advantage of TFRecords is that it is possible to store sequence data — for instance, a time series or word encodings — in a way that allows for very efficient and (from a coding perspective) convenient import of this type of data\n",
        "\n",
        "* Reference :- https://medium.com/mostly-ai/tensorflow-records-what-they-are-and-how-to-use-them-c46bc4bbb564\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2OgsTsuoxfDE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import argparse\n",
        "import os\n",
        "import sys\n",
        "\n",
        "import tarfile\n",
        "from six.moves import cPickle as pickle\n",
        "from six.moves import xrange  # pylint: disable=redefined-builtin\n",
        "import tensorflow as tf\n",
        "\n",
        "CIFAR_FILENAME = 'cifar-10-python.tar.gz'\n",
        "CIFAR_DOWNLOAD_URL = 'https://www.cs.toronto.edu/~kriz/' + CIFAR_FILENAME\n",
        "CIFAR_LOCAL_FOLDER = 'cifar-10-batches-py'\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HltRSeujxja4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def download_and_extract(data_dir):\n",
        "  # download CIFAR-10 if not already downloaded.\n",
        "  tf.contrib.learn.datasets.base.maybe_download(CIFAR_FILENAME, data_dir,\n",
        "                                                CIFAR_DOWNLOAD_URL)\n",
        "  tarfile.open(os.path.join(data_dir, CIFAR_FILENAME),\n",
        "               'r:gz').extractall(data_dir)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zPfgtGJjxm_M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _int64_feature(value):\n",
        "  return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ulLv0XNxpu9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _bytes_feature(value):\n",
        "  return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0jsCGx7UxsNx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _get_file_names():\n",
        "  \"\"\"Returns the file names expected to exist in the input_dir.\"\"\"\n",
        "  file_names = {}\n",
        "  file_names['train'] = ['data_batch_%d' % i for i in xrange(1, 6)]\n",
        "  file_names['validation'] = ['test_batch']\n",
        "  # file_names['eval'] = ['test_batch']\n",
        "  return file_names\n",
        "\n",
        "\n",
        "def read_pickle_from_file(filename):\n",
        "  with tf.gfile.Open(filename, 'rb') as f:\n",
        "    if sys.version_info >= (3, 0):\n",
        "      data_dict = pickle.load(f, encoding='bytes')\n",
        "    else:\n",
        "      data_dict = pickle.load(f)\n",
        "  return data_dict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eKA1qstWxw4d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convert_to_tfrecord(input_files, output_file):\n",
        "  \"\"\"Converts a file to TFRecords.\"\"\"\n",
        "  print('Generating %s' % output_file)\n",
        "  with tf.python_io.TFRecordWriter(output_file) as record_writer:\n",
        "    for input_file in input_files:\n",
        "      data_dict = read_pickle_from_file(input_file)\n",
        "      data = data_dict[b'data'];print(\"data_dict\",len(data_dict))\n",
        "      labels = data_dict[b'labels']\n",
        "      num_entries_in_batch = len(labels)\n",
        "      for i in range(num_entries_in_batch):\n",
        "        example = tf.train.Example(features=tf.train.Features(\n",
        "            feature={\n",
        "                'image': _bytes_feature(data[i].tobytes()),\n",
        "                'label': _int64_feature(labels[i])\n",
        "            }))\n",
        "        record_writer.write(example.SerializeToString())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MrDSvI6Qx1Iu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def main(data_dir):\n",
        "  print('Download from {} and extract.'.format(CIFAR_DOWNLOAD_URL))\n",
        "  download_and_extract(data_dir)\n",
        "  file_names = _get_file_names()\n",
        "  input_dir = os.path.join(data_dir, CIFAR_LOCAL_FOLDER)\n",
        "  for mode, files in file_names.items():\n",
        "    input_files = [os.path.join(input_dir, f) for f in files]\n",
        "    output_file = os.path.join(data_dir, mode + '.tfrecords')\n",
        "    try:\n",
        "      os.remove(output_file)\n",
        "    except OSError:\n",
        "      pass\n",
        "    # Convert to tf.train.Example and write the to TFRecords.\n",
        "    convert_to_tfrecord(input_files, output_file)\n",
        "  print('Done!')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yVYLEwLQx49m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if __name__ == '__main__':\n",
        "  parser = argparse.ArgumentParser()\n",
        "  parser.add_argument(\n",
        "      '--data-dir',\n",
        "      type=str,\n",
        "      default='',\n",
        "      help='Directory to download and extract CIFAR-10 to.')\n",
        "\n",
        "  args = parser.parse_args()\n",
        "  main(args.data_dir)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}